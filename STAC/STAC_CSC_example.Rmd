---
editor_options: 
  markdown: 
    wrap: 72
---

# Example how to use Paituli STAC API from R

This example shows how to use Paituli STAC (Spatio-Temporal Asset Catalog) from R, with rstac, gdalcubes and terra libraries. This workflow is suitable for processing big raster datasets, inc time series.

The main idea is to first define the search and processing as process graph. The data downloading and processing is done lazily at the end, so that only needed data (good enough cloud-free image, only needed bands and area) is downloaded.

The libraries take care of data download, so you do not need to know about file paths.

These tools work best when data is provided as Cloud-optimized GeoTiffs (COGs).

The main steps:

* Query STAC catalog, to see which data collections are available.
* Query STAC catalogue to find data from area and time of interest.
* Plot search results as index map to see their coverage.
* Create datacube, defining required bands and bbox, and time step for aggregation.
* Finally, calculate the result and plot it and save it to file.
* Additional example is given, how to get data URLs and use them with terra package.

This example is partly based on:

* https://r-spatial.org/r/2021/04/23/cloud-based-cubes.html
* https://geocompr.robinlovelace.net/gis.html#cloud
* <https://cran.r-project.org/web/packages/rstac/vignettes/rstac-03-cql2-mpc.html>

## Import required libraries.


```{r}
library(rstac) # For working with STAC catalogs, rstac docs: <https://rdrr.io/cran/rstac/> 
library(httr) # For adding response type to STAC requests 
library(gdalcubes) # For 4D spatial data cube in R
library(sf) # For plotting index map 
library(tidyverse) # For plotting
library(ggplot2) # For plotting data
library(terra) # For opening single raster datasets
```

Set first R working directory, select any suitable directory for you.

```{r}
#setwd('/scratch/project_2000XXX/yyy')
```

Set up STAC API endpoint

```{r}
stac_URL <- "https://paituli.csc.fi/geoserver/ogc/stac"
```

Define the center of area of interest, in this case Helsinki.
For searching STAC always geographic coordinates must be used (EPSG=4326).

```{r}
hki_wgs_x <- 24.945 
hki_wgs_y <- 60.173
```

Convert to CQL2 BBOX, used for search.

```{r}
bbox <- c(hki_wgs_x-0.1, hki_wgs_y-0.1, hki_wgs_x+0.1, hki_wgs_y+0.1)
area_of_interest <- cql2_bbox_as_geojson(bbox)
```

Convert to SF dataframe, used for plotting and later creating data cube.

```{r}
helsinki_wgs <- data.frame(x = hki_wgs_x, y = hki_wgs_y) |>
  st_as_sf(coords = 1:2, crs = 4326)
```

Define time period for search.

```{r}
startDate <- "2021-08-01" 
endDate <- "2021-09-30"
```


## STAC basics

Get connection to STAC catalog.

```{r}
stac_gs <- stac(stac_URL, force_version = '1.0.0')
```

List all collections.

```{r}
stac_gs |> 
  collections() |> 
  get_request()
```

Define the collection used in this example.

```{r}
collectionName <- 'sentinel_1_11_days_mosaics_at_fmi'
```

Get info about a specific catalog.

```{r}
stac_gs |> 
  collections(collectionName) |> 
  get_request()
```
What kind of data (=assets) does a collection have? Paituli STAC provides as default html as result, therefore result type has to manually be added to this and several other requests.

```{r}
stac_gs |> 
  stac_search( collections = collectionName, limit = 1) |>
  get_request(accept("application/geo+json")) |> 
  items_assets()
```

## Search items
### Basic search.

```{r}
stac_items <- stac_gs |> 
  stac_search( collections = collectionName, bbox = bbox, datetime =
    paste(startDate,"/",endDate, sep="")) |>
  get_request(verbose(),
  accept("application/geo+json")) |>
  items_fetch(accept("application/geo+json"))
```

* If modifying the search criteria, it might be better first to exclude the items_fetch part and check first how many items were found. Fetching big amounts of items is rather slow.
* You may remove verbose() from above, if you do not want to see what requests are actually done.

How many items were found?

```{r}
stac_items
items_matched(stac_items)
```

### Advanced search with filter

Basic search supports only limiting by collection name, location and time. `sentinel2-l2a` collection has also information about cloud coverage. This can be searched with filter search.

See https://rdrr.io/cran/rstac/man/ext_filter.html for more options.

```{r}
stac_items2 <- stac_gs |> 
  ext_filter( collection == "sentinel2-l2a" 
              #&& s_intersects(geometry, {{area_of_interest}}) 
              && datetime >= {{startDate}} 
              && datetime <= {{endDate}} 
              && `eo:cloud_cover` > 80 ) |> 
  post_request(verbose(), accept("application/geo+json")) |>
  items_fetch(accept("application/geo+json"))

items_matched(stac_items2)

```


## Plotting index map of search results

Convert search results to sf geodataframe.

```{r}
stac_items_sf <- items_as_sf(stac_items2)
stac_items_sf
```


Plot, also the data search point, just as example. Using alpha to visualize overlapping polygons.

```{r}
ggplot() + geom_sf(data = stac_items_sf, fill=alpha("violet",0.1)) +
  geom_sf(data = helsinki_wgs)
```

## Create data cube and analyze the data

`gdalcubes` is the package to use for 4D spatial data cubes in R. It is somewhat different than `xarray` in Python, and it likely has less features (and other users).

gdalcubes docs: https://rdrr.io/cran/gdalcubes/

Create an image collection of files found with STAC. The image collection could be saved also to disc, so that it is not necessary to repeat the STAC search again.

See https://rdrr.io/cran/gdalcubes/man/create_image_collection.html for more options

```{r}

assets = c("mean_vv")
col <- stac_image_collection(stac_items$features, asset_names = assets)
```

Create data cube view. In what coordinate system you want to have the data? It can be different that the original data, but using same coordinate system than data has, is fastest. 

You can see data coordinate system for example from this. Note, one collection may have data with different coordinate systems.

```{r}
( stac_items_sf <- items_as_sf(stac_items) )
```

```{r}
data_crs <- 3067 #data_crs = 32635
```

Used other options:
* Extent.
* dx and dy - pixel size, it can be different than original data, for bigger pixel data overviews will be used, if available.
* dt - time interval, "P1M" means monthly
* aggregation - how data is aggregated

More options: https://rdrr.io/cran/gdalcubes/man/cube_view.html

Extent is given in the same coordinate system as used for data cube.
For extent, convert Helsinki coordinates to data coordinate system.

```{r}
helsinki_utm <- st_transform(helsinki_wgs, data_crs) 
helsinki_utm_x <- st_coordinates(helsinki_utm)[1] 
helsinki_utm_y <- st_coordinates(helsinki_utm)[2] 
buffer_size <- 10000

v <- cube_view(srs = paste("EPSG:",data_crs, sep=""), 
               extent = list(t0 = startDate, 
                             t1 = endDate, 
                             left = helsinki_utm_x - buffer_size, 
                             right = helsinki_utm_x + buffer_size, 
                             top = helsinki_utm_y + buffer_size, 
                             bottom = helsinki_utm_y - buffer_size), 
               dx = 60, 
               dy = 60, 
               dt ="P1M", 
               aggregation = "mean")
v
```

Create data cube with given settings and data from above image collection. `gdalcubes` provides additional functions for analyzing the data. 

```{r}
cube <- raster_cube(col, v)
```

Plot data, monthly aggregation by mean was defined already in the cube view level, so here we only plot now. More options: https://gdalcubes.github.io/source/reference/ref/plot.cube.html

```{r}
plot(cube, key.pos=4, zlim=c(-40,30), t = 1:2, ncol = 1)
```

Write files, gdalcubes writes automatically each time period to own file. The files are created to the folder `tifs`, check from the file browser.
More options: <https://gdalcubes.github.io/source/reference/ref/write_tif.html>

```{r}
write_tif(cube, dir='tifs', prefix='monthly_mean_bbox_')
```

### The same data-cube for whole Finland.

For big areas or long time series gdalcubes supports parallel processing to some extent. In a test with all Finland 8 cores was ~5x times faster than 1 core. Set options according to available number of cores. This can be run with one core, but it takes a few minutes. In Puhti, you need to reserve the cores at the RStudio start-up time.

```{r}
gdalcubes_options(parallel=1)
```

Create data view with full extent of original data. For testing increase pixel size significantly, to first check that results are as expected.

```{r}
v <- cube_view(srs = paste("EPSG:",data_crs, sep=""), 
               extent = col, 
               dx = 60, 
               dy = 60, 
               dt ="P1M", 
               aggregation = "mean")
```

Calculate and save to files. With one core this takes a few minutes.

```{r}
#raster_cube(col, v) |> 
#  write_tif( dir='tifs', prefix='monthly_mean_Finland')
```


## Using data from STAC with terra

Sometimes it might be useful to open the files with `terra` or some other R package for analysis not supported with `gdalcubes`. Terra does not support time dimension, so handling that must be done manually.

Get the data URL of second item found. Add vsicurl to URL, so that terra could use it. 

```{r}
href <- stac_items$features[[2]]$assets$mean_vv$href # 
href_with_vsicurl <- paste('/vsicurl/',href, sep="")
```

Open dataset from URL, note that only some metadata is fetched at this point.

```{r}
data <- terra::rast(href_with_vsicurl)
# It is possible to read only a subset of data 
data_cropped <- terra::crop(data, ext(370000, 400000, 6665000, 6685000))
```

Plot, now data is fetched and plotted.

```{r}
plot(data_cropped)
```
